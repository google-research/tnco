{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf69faa-7599-45f4-bf1c-b24902d51b71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:17:45.595629Z",
     "iopub.status.busy": "2025-10-22T14:17:45.595285Z",
     "iopub.status.idle": "2025-10-22T14:17:48.990866Z",
     "shell.execute_reply": "2025-10-22T14:17:48.989520Z",
     "shell.execute_reply.started": "2025-10-22T14:17:45.595587Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from qiskit.circuit.random import random_circuit as qiskit_random_circuit\n",
    "from tnco.app import Optimizer, Tensor, TensorNetwork\n",
    "import tnco.utils.tn as tn_utils\n",
    "import more_itertools as mit\n",
    "from random import Random\n",
    "import quimb.tensor as qt\n",
    "import itertools as its\n",
    "import functools as fts\n",
    "import operator as op\n",
    "import numpy as np\n",
    "import qiskit\n",
    "import cirq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da4a9e-e1cb-4083-81e5-bf3fc13cb898",
   "metadata": {},
   "source": [
    "## Initialize Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419f9fee-7426-40e3-80ec-110214ccc55f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:17:48.994040Z",
     "iopub.status.busy": "2025-10-22T14:17:48.993205Z",
     "iopub.status.idle": "2025-10-22T14:17:49.086797Z",
     "shell.execute_reply": "2025-10-22T14:17:49.085492Z",
     "shell.execute_reply.started": "2025-10-22T14:17:48.994003Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The optimizer can be initialized and reused multiple times. The optimization\n",
    "# method can be specified using 'method' (the default method is simulated\n",
    "# annealing, i.e. 'method=sa'). If 'max_width' is not provided, no memory\n",
    "# constraints are enforced. If 'max_width' is provided, slices are added to the\n",
    "# contraction to make sure that every intermediate tensor during the\n",
    "# contraction will be within 'max_width' after the slices are applied.\n",
    "\n",
    "# Initialize Optimizer (infinite memory)\n",
    "opt = Optimizer()\n",
    "\n",
    "# Initialize Optimizer (finite width)\n",
    "opt_fw = Optimizer(max_width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c092943-de17-4779-b2f7-0215a467172a",
   "metadata": {},
   "source": [
    "## Optimize `cirq.Circuit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e137412-a6cf-49a0-bb85-3a4bb978abf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:17:49.088668Z",
     "iopub.status.busy": "2025-10-22T14:17:49.088323Z",
     "iopub.status.idle": "2025-10-22T14:17:49.145410Z",
     "shell.execute_reply": "2025-10-22T14:17:49.143954Z",
     "shell.execute_reply.started": "2025-10-22T14:17:49.088639Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'tnco.app.Optimizer' accepts multiple format, including 'cirq.Circuit'.\n",
    "# See: 'tnco.app.load_tn' for more informations.\n",
    "\n",
    "# Get a random circuit\n",
    "circuit = cirq.testing.random_circuit(qubits=8, n_moments=16, op_density=1)\n",
    "\n",
    "# Get all qubits\n",
    "qubits = sorted(circuit.all_qubits())\n",
    "\n",
    "# The optimizer automatically simplify circuits and automatically decompose\n",
    "# potential hyper-indices. To keep the final state simple without any\n",
    "# hyper-index, let's add an initial / final layer of H**0.5, to avoid trivial\n",
    "# simplifications.\n",
    "circuit = (cirq.H**0.5).on_each(qubits) + circuit + (cirq.H**\n",
    "                                                     0.5).on_each(qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ebaecc-c0f4-4777-8757-d1e0ed624f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:17:49.146970Z",
     "iopub.status.busy": "2025-10-22T14:17:49.146684Z",
     "iopub.status.idle": "2025-10-22T14:17:55.626042Z",
     "shell.execute_reply": "2025-10-22T14:17:55.624431Z",
     "shell.execute_reply.started": "2025-10-22T14:17:49.146945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The optimizer returns the tensor network used for the optimization, and the\n",
    "# results of the optimization. If the provided tensor network is a circuit, the\n",
    "# initial / final state can be provided. By default, the initial / final state\n",
    "# are set to |0>...|0>. Otherwise, the initial / final state can be provided\n",
    "# for each qubit as a 'dict'. If a qubit does not appear, it is considered as\n",
    "# an open qubit. If initial / final state is None, all qubits are open.\n",
    "# See 'tnco.app.load_tn' for more informations.\n",
    "\n",
    "# Results assuming infinite memory\n",
    "tn, res = opt.optimize(circuit,\n",
    "                       betas=(0, 1e5),\n",
    "                       initial_state='+',\n",
    "                       final_state=None,\n",
    "                       n_steps=1_000,\n",
    "                       n_runs=4)\n",
    "\n",
    "# Results enforcing finite width\n",
    "tn_fw, res_fw = opt_fw.optimize(circuit,\n",
    "                                betas=(0, 1e5),\n",
    "                                initial_state='+',\n",
    "                                final_state=None,\n",
    "                                n_steps=1_000,\n",
    "                                n_runs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a0bfc9-2682-4b74-812b-e0d84787472b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:17:55.628813Z",
     "iopub.status.busy": "2025-10-22T14:17:55.627894Z",
     "iopub.status.idle": "2025-10-22T14:17:55.676619Z",
     "shell.execute_reply": "2025-10-22T14:17:55.674982Z",
     "shell.execute_reply.started": "2025-10-22T14:17:55.628760Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Infinite Memory| Runtime: 0.0044s\n",
      "# Infinite Memory| log10(FLOP): 3.7\n",
      "# -\n",
      "# Finite Width| Runtime: 0.0044s\n",
      "# Finite Width| log10(FLOP): 6.1\n",
      "# Finite Width| Number of sliced indices: 14\n"
     ]
    }
   ],
   "source": [
    "# ContractionResults contain useful informations regarding the optimal contraction:\n",
    "# - .cost: The contraction cost in floating-point-operations (FLOP)\n",
    "# - .path: The contraction path\n",
    "# - .runtime_s: The total runtime spent for the optimization\n",
    "#\n",
    "# In case the tensor network to optimize is composed of disconnected components,\n",
    "# the cost / path for each connected component is also provided:\n",
    "# - .disconnected_costs: The contraction cost in FLOP for each connected\n",
    "#                        component\n",
    "# - .disconnected_paths: The contraction path for each connected component\n",
    "#\n",
    "# If 'max_width' is provided, the cost would reflect the presence of slices.\n",
    "# ContractionResults will also include the sliced indices:\n",
    "# - .slices: Sliced indices\n",
    "# - .disconnected_sliced: Sliced indices for each connected component\n",
    "\n",
    "# Sort accordingly to the cost\n",
    "res = sorted(res, key=lambda x: x.cost)\n",
    "res_fw = sorted(res_fw, key=lambda x: x.cost)\n",
    "\n",
    "# Print stats\n",
    "print('# Infinite Memory| Runtime: {:1.2g}s'.format(\n",
    "    sum(map(lambda x: x.runtime_s, res)) / len(res)))\n",
    "print('# Infinite Memory| log10(FLOP): {:1.2g}'.format(res[0].cost.log10()))\n",
    "print('# -')\n",
    "print('# Finite Width| Runtime: {:1.2g}s'.format(\n",
    "    sum(map(lambda x: x.runtime_s, res)) / len(res)))\n",
    "print('# Finite Width| log10(FLOP): {:1.2g}'.format(res_fw[0].cost.log10()))\n",
    "print('# Finite Width| Number of sliced indices: {}'.format(\n",
    "    len(res_fw[0].slices)))\n",
    "\n",
    "# Get contraction path (infinite memory)\n",
    "path = res[0].path\n",
    "\n",
    "# Get contraction path (finite width)\n",
    "path_fw = res_fw[0].path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa0238c-9f24-4bfe-bc7d-3009031effb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:17:55.678307Z",
     "iopub.status.busy": "2025-10-22T14:17:55.677978Z",
     "iopub.status.idle": "2025-10-22T14:17:55.741161Z",
     "shell.execute_reply": "2025-10-22T14:17:55.739852Z",
     "shell.execute_reply.started": "2025-10-22T14:17:55.678275Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The resulting contraction can be contracted using your favorite library.\n",
    "# Ouput indices in the tensor network are named '(qubit_name, i)', with\n",
    "# 'qubit_name' being the name of the qubit, and 'i' is zero only if 'qubit_name'\n",
    "# belongs to the initial state.\n",
    "\n",
    "# Get the exact final state\n",
    "exact_final_state = cirq.Simulator().simulate(\n",
    "    circuit,\n",
    "    initial_state=np.ones(2**len(qubits)) / np.sqrt(2**len(qubits)),\n",
    "    qubit_order=qubits).state_vector()\n",
    "\n",
    "# Get the final state\n",
    "final_state = qt.TensorNetwork(map(qt.Tensor, tn.arrays, tn.ts_inds)).contract(\n",
    "    optimize=path, output_inds=tn.output_inds)\n",
    "\n",
    "# No qubit should belong to the initial state\n",
    "assert all(map(lambda x: x != 0, map(op.itemgetter(1), final_state.inds)))\n",
    "\n",
    "# Re-index output inds for a quicker access\n",
    "final_state = final_state.reindex(dict(\n",
    "    zip(final_state.inds, map(op.itemgetter(0), final_state.inds))),\n",
    "                                  inplace=True)\n",
    "\n",
    "# Transpose final state accordingly to qubits\n",
    "final_state = final_state.transpose(*qubits, inplace=True)\n",
    "\n",
    "# Check\n",
    "np.testing.assert_allclose(final_state.data.ravel(),\n",
    "                           exact_final_state,\n",
    "                           atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c699cf2-8f31-4d37-a157-2f8db9fe6492",
   "metadata": {},
   "source": [
    "## Optimize `qiskit.QuantumCircuit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e2e4c1-99a3-42fc-aa65-abafdee139dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:17:55.744431Z",
     "iopub.status.busy": "2025-10-22T14:17:55.744013Z",
     "iopub.status.idle": "2025-10-22T14:17:58.684500Z",
     "shell.execute_reply": "2025-10-22T14:17:58.682785Z",
     "shell.execute_reply.started": "2025-10-22T14:17:55.744399Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Similarly, the optimizer can use 'qiskit.QuantumCircuit' as input\n",
    "\n",
    "# Implement sqrt of Hadamard\n",
    "sqrt_H = qiskit.circuit.library.UnitaryGate(cirq.unitary(cirq.H**0.5),\n",
    "                                            label='√H')\n",
    "\n",
    "# Get a random circuit\n",
    "qiskit_circuit = qiskit.QuantumCircuit(8)\n",
    "mit.consume(map(lambda i: qiskit_circuit.append(sqrt_H, [i]), range(8)))\n",
    "qiskit_circuit = qiskit_circuit.compose(qiskit_random_circuit(8, 16))\n",
    "mit.consume(map(lambda i: qiskit_circuit.append(sqrt_H, [i]), range(8)))\n",
    "\n",
    "# Optimize circuit\n",
    "tn, res = opt.optimize(qiskit_circuit,\n",
    "                       betas=(0, 1e5),\n",
    "                       initial_state='+',\n",
    "                       final_state=None,\n",
    "                       n_steps=1_000,\n",
    "                       n_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc31ee2-830e-4dfe-8f8b-fc9cdf3e2d8d",
   "metadata": {},
   "source": [
    "## Optimize Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c688e8a4-21ab-44db-ad03-2527e474ba03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:17:58.686308Z",
     "iopub.status.busy": "2025-10-22T14:17:58.685998Z",
     "iopub.status.idle": "2025-10-22T14:18:01.604216Z",
     "shell.execute_reply": "2025-10-22T14:18:01.602539Z",
     "shell.execute_reply.started": "2025-10-22T14:17:58.686278Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instead of using 'cirq.Circuit', it is possible to directly provide a list of\n",
    "# matrices with their indices.\n",
    "gates = list(\n",
    "    map(lambda gate: (cirq.unitary(gate), gate.qubits),\n",
    "        circuit.all_operations()))\n",
    "\n",
    "# Optimize the tensor network\n",
    "tn, res = opt.optimize(gates,\n",
    "                       betas=(0, 1e5),\n",
    "                       initial_state='+',\n",
    "                       final_state=None,\n",
    "                       n_steps=1_000,\n",
    "                       n_runs=4)\n",
    "\n",
    "# Get path\n",
    "path = sorted(res, key=lambda x: x.cost)[0].path\n",
    "\n",
    "# Get the final state\n",
    "final_state = qt.TensorNetwork(map(qt.Tensor, tn.arrays, tn.ts_inds)).contract(\n",
    "    optimize=path, output_inds=tn.output_inds)\n",
    "\n",
    "# No qubit should belong to the initial state\n",
    "assert all(map(lambda x: x != 0, map(op.itemgetter(1), final_state.inds)))\n",
    "\n",
    "# Re-index output inds for a quicker access\n",
    "final_state = final_state.reindex(dict(\n",
    "    zip(final_state.inds, map(op.itemgetter(0), final_state.inds))),\n",
    "                                  inplace=True)\n",
    "\n",
    "# Transpose final state accordingly to qubits\n",
    "final_state = final_state.transpose(*qubits, inplace=True)\n",
    "\n",
    "# Check\n",
    "np.testing.assert_allclose(final_state.data.ravel(),\n",
    "                           exact_final_state,\n",
    "                           atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a1bd8-c983-48c1-b813-a7db10a49dd7",
   "metadata": {},
   "source": [
    "## Optimize Arbitrary Tensor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e50711-1424-4854-91d2-d8d083c48166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:18:01.605643Z",
     "iopub.status.busy": "2025-10-22T14:18:01.605322Z",
     "iopub.status.idle": "2025-10-22T14:18:04.514317Z",
     "shell.execute_reply": "2025-10-22T14:18:04.512824Z",
     "shell.execute_reply.started": "2025-10-22T14:18:01.605612Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/smandra/projects/tnco_v2_clean/tnco/app/app.py:318: UserWarning: Cannot decompose hyper-indices if not all arrays are provided.\n",
      "  warn(\"Cannot decompose hyper-indices if not \"\n"
     ]
    }
   ],
   "source": [
    "# An arbitrary tensor tensor can be specified by using 'tnco.app.TensorNetwork'\n",
    "from tnco.tests.utils import generate_random_tensors\n",
    "\n",
    "# Get a list of random indices for each tensor. The resulting tensor network\n",
    "# will have two disconnected components, 2 * 10 tensors (10 tensors for each\n",
    "# connected component), 2 * 3 output indices (2 indices for each connected\n",
    "# component),  and every index will be a hyper-index of degree 3.\n",
    "ts_inds, output_inds = generate_random_tensors(n_tensors=10,\n",
    "                                               n_inds=20,\n",
    "                                               n_cc=2,\n",
    "                                               k=3,\n",
    "                                               n_output_inds=3)\n",
    "\n",
    "# Get all indices\n",
    "inds = frozenset(mit.flatten(ts_inds))\n",
    "\n",
    "# Get some random dimensions\n",
    "dims = dict(zip(inds, Random().choices(range(2, 4), k=len(inds))))\n",
    "\n",
    "# There should be 2 * 10 tensors\n",
    "assert len(ts_inds) == 2 * 10\n",
    "\n",
    "# There should be 2 * 20 indices\n",
    "assert len(inds) == 2 * 20\n",
    "\n",
    "# There should be 2 * 3 open indices\n",
    "assert len(output_inds) == 2 * 3\n",
    "\n",
    "# Get tensor network. For the optimization, arrays are not required\n",
    "tn = TensorNetwork(map(\n",
    "    lambda id, xs: Tensor(xs, map(dims.get, xs), tags=dict(id=id)), its.count(),\n",
    "    ts_inds),\n",
    "                   output_inds=output_inds)\n",
    "\n",
    "# Optimize the tensor network. Because the tensor network is simplified before\n",
    "# being optimized, the output 'tn' might be different from the input 'tn'. Also,\n",
    "# if arrays are not provided, the decomposition of hyper indices is\n",
    "# automatically disabled\n",
    "new_tn, res = opt.optimize(tn, fuse=10, betas=(0, 1e5), n_steps=1_000, n_runs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c37c99-898c-4be8-b000-457fb7aa2610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:18:04.516512Z",
     "iopub.status.busy": "2025-10-22T14:18:04.516054Z",
     "iopub.status.idle": "2025-10-22T14:18:04.577039Z",
     "shell.execute_reply": "2025-10-22T14:18:04.575644Z",
     "shell.execute_reply.started": "2025-10-22T14:18:04.516479Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Given the optimized path, we can later contract the actual tensors\n",
    "arrays = list(map(lambda t: np.random.normal(size=t.dims), tn.tensors))\n",
    "\n",
    "# Fused tensors are kept track in 'TensorNetwork.tags['fuse_path']'\n",
    "ts_inds, output_inds, new_arrays = tn_utils.contract(new_tn.tags['fuse_path'],\n",
    "                                                     ts_inds=tn.ts_inds,\n",
    "                                                     output_inds=tn.output_inds,\n",
    "                                                     dims=tn.dims,\n",
    "                                                     arrays=arrays)\n",
    "\n",
    "# Check consistency with new_tn\n",
    "assert new_tn.ts_inds == tuple(ts_inds)\n",
    "assert new_tn.output_inds == output_inds\n",
    "assert tn.output_inds == new_tn.output_inds\n",
    "\n",
    "# Contract the arrays before the optimization\n",
    "final_tensor = qt.TensorNetwork(map(\n",
    "    qt.Tensor, arrays, tn.ts_inds)).contract(output_inds=tn.output_inds)\n",
    "\n",
    "# Contract the arrays after the optimization\n",
    "new_final_tensor = qt.TensorNetwork(map(\n",
    "    qt.Tensor, new_arrays, new_tn.ts_inds)).contract(\n",
    "        optimize=sorted(res, key=lambda x: x.cost)[0].path,\n",
    "        output_inds=new_tn.output_inds).transpose_like(final_tensor)\n",
    "\n",
    "# Check\n",
    "np.testing.assert_allclose(final_tensor.data, new_final_tensor.data, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc49408-85ab-4654-a873-9230c8ba04ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:18:04.578408Z",
     "iopub.status.busy": "2025-10-22T14:18:04.578122Z",
     "iopub.status.idle": "2025-10-22T14:18:04.618551Z",
     "shell.execute_reply": "2025-10-22T14:18:04.617595Z",
     "shell.execute_reply.started": "2025-10-22T14:18:04.578382Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The optimizer also accepts a list of indices of the format:\n",
    "#\n",
    "# [\n",
    "#  (dim_1, tensor_1_1, tensor_1_2, ... tensor_1_k1),\n",
    "#  ...\n",
    "#  (dim_n, tensor_n_1, tensor_n_2, ... tensor_n_kn)\n",
    "# ]\n",
    "#\n",
    "# Open indices can be specified using the token (by default: '*'):\n",
    "#\n",
    "# [\n",
    "#  (dim_1, tensor_1_1, tensor_1_2, ... tensor_1_k1, '*'),\n",
    "#  ...\n",
    "#  (dim_n, tensor_n_1, tensor_n_2, ... tensor_n_kn)\n",
    "# ]\n",
    "#\n",
    "# See 'tnco.app.load_tn' and 'tnco.utils.tn.read_inds' for more informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "373ed2b8-335c-45fd-81f3-463d3577382c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:18:04.619841Z",
     "iopub.status.busy": "2025-10-22T14:18:04.619571Z",
     "iopub.status.idle": "2025-10-22T14:18:07.500993Z",
     "shell.execute_reply": "2025-10-22T14:18:07.500044Z",
     "shell.execute_reply.started": "2025-10-22T14:18:04.619818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a map of indices. To track the location in 'arrays' of the tensors,\n",
    "# let's enumerate them and use it as name of the tensor.\n",
    "inds_map = dict(zip(dims, map(lambda d: [d], dims.values())))\n",
    "for idx, tensor in enumerate(tn.tensors):\n",
    "    for x in tensor.inds:\n",
    "        inds_map[x].append(idx)\n",
    "\n",
    "# Add token to output indices\n",
    "output_index_token = '*'\n",
    "for x in tn.output_inds:\n",
    "    inds_map[x].append(output_index_token)\n",
    "\n",
    "# Optimize the tensor network\n",
    "new_tn, res = opt.optimize(inds_map.values(),\n",
    "                           betas=(0, 1e5),\n",
    "                           n_steps=1_000,\n",
    "                           output_index_token=output_index_token,\n",
    "                           n_runs=4)\n",
    "\n",
    "# Because indices don't have any name, they are named using the order in which\n",
    "# they appear in 'inds_map'\n",
    "assert frozenset(map(fts.partial(mit.nth, inds_map),\n",
    "                     new_tn.output_inds)) == tn.output_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40d60455-d4a8-45ac-936d-8c337451efd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T14:18:07.503493Z",
     "iopub.status.busy": "2025-10-22T14:18:07.503003Z",
     "iopub.status.idle": "2025-10-22T14:18:07.601253Z",
     "shell.execute_reply": "2025-10-22T14:18:07.600391Z",
     "shell.execute_reply.started": "2025-10-22T14:18:07.503455Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# An easier way would be to load the tensor network first, and then optimize\n",
    "# it. Because we used a list of indices, the order of the tensors and the\n",
    "# indices might have changed.\n",
    "new_tn_map, new_dims, new_output_inds, new_sparse_inds = tn_utils.read_inds(\n",
    "    inds_map, output_index_token=output_index_token)\n",
    "new_ts_inds = list(map(op.itemgetter(1), sorted(new_tn_map.items())))\n",
    "\n",
    "# Get the new arrays\n",
    "new_arrays = list(\n",
    "    map(lambda a, old_xs, new_xs: qt.Tensor(a, old_xs).transpose(*new_xs).data,\n",
    "        arrays, tn.ts_inds, new_ts_inds))\n",
    "\n",
    "# Initialize the new tensor network to optimize\n",
    "new_tn = TensorNetwork(map(lambda xs, a: Tensor(xs, array=a), new_ts_inds,\n",
    "                           new_arrays),\n",
    "                       output_inds=new_output_inds)\n",
    "\n",
    "# Check consistency\n",
    "assert all(\n",
    "    map(lambda xs, ys: frozenset(xs) == frozenset(ys), tn.ts_inds,\n",
    "        new_tn.ts_inds))\n",
    "assert tn.output_inds == new_tn.output_inds\n",
    "assert tn.sparse_inds == new_tn.sparse_inds\n",
    "assert tn.dims == new_tn.dims\n",
    "\n",
    "# Optimize it\n",
    "new_tn, res = opt.optimize(new_tn,\n",
    "                           fuse=10,\n",
    "                           betas=(0, 1e5),\n",
    "                           n_steps=1_000,\n",
    "                           n_runs=4)\n",
    "\n",
    "# Contract the tensor network using the optimized path\n",
    "new_final_tensor = qt.TensorNetwork(\n",
    "    map(qt.Tensor, new_tn.arrays,\n",
    "        new_tn.ts_inds)).contract(optimize=sorted(res,\n",
    "                                                  key=lambda x: x.cost)[0].path,\n",
    "                                  output_inds=new_tn.output_inds)\n",
    "\n",
    "# Because indices have been decomposed in hyper-inds, the output indices might be\n",
    "# different. We can re-index them using the hyper-indices decomposition map\n",
    "new_final_tensor = new_final_tensor.reindex(dict(\n",
    "    zip(map(new_tn.tags['hyper_inds_map'].get, tn.output_inds),\n",
    "        tn.output_inds)),\n",
    "                                            inplace=True)\n",
    "\n",
    "# Also, it might happen that a different decomposition of hyper indices lead to\n",
    "# the removal of one or more output indices. It happens, for instance, if only\n",
    "# one projection for the removed indices is different from zero.\n",
    "assert frozenset(new_final_tensor.inds).issubset(\n",
    "    final_tensor.inds) or frozenset(final_tensor.inds).issubset(\n",
    "        new_final_tensor.inds)\n",
    "\n",
    "# Transpose\n",
    "new_final_tensor = new_final_tensor.transpose_like(final_tensor, inplace=True)\n",
    "\n",
    "# Check\n",
    "np.testing.assert_allclose(final_tensor.data, new_final_tensor.data, atol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
